[
["index.html", "MATH-505A Screening Solutions", " MATH-505A Screening Solutions Solutions to MATH-505A screening exams are provided. They are indexed by (year, semester). "],
["spring.html", "1 2006-Spring 1.1 Paper 1.2 Problem 1 1.3 Problem 2 1.4 Problem 3", " 1 2006-Spring 1.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20061/505aspring06.pdf 1.2 Problem 1 Define an indicator variable \\(I_i\\) as: \\[ I_i = \\begin{cases} 1 &amp; \\text{if $i^{th}$ and $(i+1)^{th}$ cards are different (H,T) or (T,H)},\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Now the number of runs in a sequence of \\(n\\) coin tosses is given by: \\[ R_n = 1+ \\sum_{i=2}^{n-1} I_i \\forall n\\geq 3 \\] Thus, \\[ \\begin{align} ER_n &amp;= 1 + \\sum_{i=2}^{n-1} EI_i \\\\ &amp;= 1 + \\sum_{i=2}^{n-1} P(I_i=1) \\end{align} \\] \\(P(I_i=1)\\) is given by : \\(P(I_i=1)=p\\times q + q \\times p\\) (heads followed by tails or tails followed by heads) And hence, \\[ ER_n = 1+(n-2) \\times (2pq) \\] Check: - \\(ER_1 = 1\\) and that is \\(ER_1=1\\) - \\(P(R_2=1)=p^2 + q^2\\) and \\(P(R_2=2)=pq+qp=2pq\\) thus \\(E(R_2)=(p^2+q^2)+4pq = (p+q)^2+2pq = 1+2pq\\) which is what \\(ER_n\\) formula gives us for n=2 1.2.1 Variance Calculation: To calculate: \\(\\sigma^2=Var(R_n)\\) \\(Var(R_n) = E(R_n^2)-(ER_n)^2\\) So we focus on calculating \\(ER_n^2\\): \\[\\begin{align} ER_n^2 &amp;= E((1+\\sum_{i=2}^{n-1}I_i)^2)\\\\ &amp;= E(1+(\\sum_{i=1}^{n-1}I_i)^2 + 2\\sum_{i=2}^{n-1}I_i))\\\\ &amp;= E(1+(\\sum{i=2}^{n-1}I_i^2 + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j) + 2\\sum_{i=2}^{n-1}I_i)\\\\ &amp;= 1 + (n-2)\\times(2pq) + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j + 2(n-2)(2pq) \\\\ &amp;= 1+3(n-2)\\times(2pq) + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j\\\\ \\end{align}\\] In order to calculate \\(EI_iI_j\\), we consider following 3 cases: Case 1: \\(j-i=1\\), then \\(P(I_i=1, I_j=1)\\) = Probaility that \\(i^{th}, (i+1)^{th} \\mathrm{and} (i+2)^{th}\\) cards are different. For \\(i=2\\) to \\(i={n-1}\\) there are \\((n-2-1)=n-3\\) such terms and \\(P(I_i,I_j=1)= pqp+qpq=pq\\) Case 2: \\(j-i&gt;=2\\), then \\(P(I_i=1,I_j=1) = P(I_i=1)P(I_j=1)\\) , that is these events are independent unless they occur next to each other as in Case 1. and hence \\(P(I_i=1,I_j=1)=P(I_i=1)P(I_j=1) = (pq)^2\\) and there are \\((n-4)\\) such ways to choose Thus, \\(ER_n^2 = 1+6pq(n-2)+2\\times((n-3)\\times (pq)+(n-4)\\times(pq)^2)\\) Substitute in (skipping/TODO) \\(Var(R_n) = ER_n^2-(ER_n)^2\\) 1.3 Problem 2 \\(X = \\{X_1,X_2 \\dots, X_n \\}\\) and \\(Y=\\sum_{i=1}^{N}c_iX_i\\) To determine : - Distribution of Y We use characteristic functions Using the characteristic function of a normal RV: \\(\\phi_X(t) = E[e^{itX}] = e^{-it\\mu- \\frac{1}{2}\\sigma^2t^2}\\) For multivariate case: \\(\\phi_X(\\bf{t}) = E[e^{i\\bf{t^T}X}] = e^{-i\\bf{t^T}\\mu- \\frac{1}{2}\\bf{t^T}\\sum^2 \\bf{t}}\\) Now, \\(Y=c^TX\\) where \\(c=[c_1,c_2 \\dots c_n]\\) (\\(Y=\\sum_{i=1}^{N}c_iX_i\\)) Thus, \\[ \\begin{align} \\phi_Y(\\bf{t}) &amp;= E[e^{i\\bf{t^T}Y}]\\\\ &amp;= E[e^{i\\bf{t}c^TX}]\\\\ &amp;= \\phi_X(tc^T)\\\\ &amp;= e^{-ic^T\\bf{t}\\mu - \\frac{1}{2}\\bf{t}^Tc\\sum^2 \\bf{t}c^T} \\end{align} \\] and thus comparing with the characteristic function we started with: \\(Y \\sim N(c^T\\mu, a\\sum a^T)\\) TODO: Check again the transposes 1.4 Problem 3 TODO "],
["spring-1.html", "2 2013-Spring 2.1 Paper 2.2 Problem 1", " 2 2013-Spring 2.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20061/505aspring06.pdf 2.2 Problem 1 Given: \\(E[X|Y]=X\\ and \\ E[Y|X]=X\\) To Show: 2.2.1 Part (a): \\(P(X=Y)=1\\) \\[ E[Y]=E[E[Y|X]]=E[X]=\\mu_x \\] Thus, \\(\\mu_y=\\mu_x\\) Also, \\[ \\begin{align} Cov(X,Y) &amp;=E[XY]-E[X]E[Y]\\\\ &amp;= E[E[XY|X]]-\\mu_x^2\\\\ &amp;= E[XE[Y|X]]-\\mu_x^2\\\\ &amp;= E[X^2]-\\mu_x^2\\\\ &amp;= \\sigma_x^2 \\end{align} \\] Repeating the above with \\(E[XY]= E[E[XY|Y]]\\) would give \\(Cov(X,Y)=\\sigma_y^2\\) and hence \\(Cov(X,Y)=\\sigma_x^2=\\sigma_y^2=Var(X)\\) which implies \\(X=Y\\) or \\(P(X=Y)=1\\) Note, we implicitly used the requirement of the variance being finite[This is what is implied by the function being squared integrable: \\(\\int |f(X)|^2dx &lt; \\infty\\) "],
["spring-2.html", "3 2014-Spring 3.1 Paper 3.2 Problem 1 3.3 Problem 2", " 3 2014-Spring 3.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20141/20141_505a.pdf 3.2 Problem 1 Define \\[I_i = \\begin{cases} 1 &amp; \\text{Game $i$ and Game $i+1$ are won},\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Thus, \\[ \\begin{align*} R &amp;= \\sum_{i=1}^{m-1}I_i\\\\ ER &amp;= \\sum_{i=1}^{m-1}EI_i\\\\ Var(R) &amp;= \\sum_{i=1}^{m-1}Var(I_i) + 2\\sum_{i &lt; j}Cov(I_i,U) \\end{align*} \\] 3.2.1 Part 1.a \\[ \\begin{align*} EI_i &amp;= P(I_i=1) \\\\ &amp;= a_ia_{i+1}\\\\ ER &amp;= \\sum_i EI_i\\\\ &amp;=\\sum_{i=1}^{m-1}a_ia_{i+1} \\end{align*} \\] \\[ \\begin{align*} Var(I_i) &amp;= E[I_i^2]-E[I_i]E[I_i]\\\\ &amp;= E[I_i](1-E[I_i])\\\\ &amp;= a_ia_{i+1}(1-a_ia_{i+1}) \\end{align*} \\] \\[ \\begin{align*} Cov(I_i,I_j) &amp;= \\begin{cases} 0 &amp; j-i\\geq 2(i&lt;j)\\\\ E[I_{i}I_{i+1}]-E[I_i]E[I_{i+1}] &amp; \\text{otherwise}\\\\ \\end{cases}\\\\ &amp;= \\begin{cases} 0 &amp; j-i\\geq 2(i&lt;j)\\\\ a_ia_{i+1}a_{i+2}(1-a_{i+1}) &amp; \\text{otherwise}\\\\ \\end{cases}\\\\ \\end{align*} \\] Thus, \\[ Var(R) = \\sum_{i=1}^{m-1}a_ia_{i+1} + 2\\sum_{i=1}^{m-2} a_ia_{i+1}a_{i+2}(1-a_{i+1}) \\] 3.2.2 Part 1.b When \\(a_n=p=0.1\\) \\[ \\begin{align*} ER &amp;= \\sum_{i=1}^{m-1}p^2\\\\ &amp;= (m-1)p^2\\\\ Var(R) &amp;= \\sum_{i=1}^{m-1}a_ia_{i+1} + 2\\sum_{i=1}^{m-2} a_ia_{i+1}a_{i+2}(1-a_{i+1})\\\\ &amp;= (m-1)p^2+2(m-2)p^3(1-p) \\end{align*} \\] 3.3 Problem 2 \\[f(x) = x/2 \\text{ for } 0 &lt; x &lt;2\\] 3.3.1 Part 2.a \\(S=X_1+X_2\\) Using convolution theorem: \\[ \\begin{align*} f_S(s) &amp;= \\int \\frac{x}{2} \\frac{s-x}{2} dx\\\\ &amp;0 &lt; s-x &lt; 2\\\\ &amp; 0 &lt;x&lt;2\\\\ &amp;\\implies s-2 &lt; x &lt; s \\text{ and } 0 &lt; x &lt; 2\\\\ &amp;\\text{Case 1: } 0 &lt; s &lt; 2\\ f_S(s) = \\int_0^s \\frac{x}{2} \\frac{s-x}{2} dx = \\frac{s^3}{24}\\\\ &amp;\\text{ Case 2: } 2 &lt; s &lt; 4\\ f_S(s) = \\int_{s-2}^4 \\frac{x}{2} \\frac{s-x}{2} dx = \\frac{1}{4}(\\frac{s(4s-s^2)}{2}-\\frac{(4^3-(s-2)^3)}{3})\\\\ \\end{align*} \\] 3.3.2 Part 2.b \\[L = min(X_1, X_2, X_3, \\dots X_{100})\\] $F_L = 1 - P(min(X_i) y) = 1 - _i P(X_i y) = 1-(1-P(X_iy)) = 1- (1-)^n $ 3.3.3 Part 2.c \\[R=X_1/X_2\\] \\[ \\begin{align*} P(X_1/X_2 \\leq z) &amp;= P(X_1 \\leq zX_2)\\\\ &amp;= \\int_0^{min(zx_2,2)} \\frac{z^2x_2^2}{4}x_2 dx_2 \\end{align*} \\] "],
["spring-3.html", "4 2017-Spring 4.1 Paper 4.2 Problem 1 4.3 Problem 2 4.4 Problem 3 4.5 Part a 4.6 Part b 4.7 Part c 4.8 Problem 4", " 4 2017-Spring 4.1 Paper 4.2 Problem 1 Let the three points be \\((x_1,y_1); (x_2,y_2); (x_3,y_3)\\) Expected area of the rectangle with sides parallel to the coordinate axes is: \\(E[\\big(\\max{(x_1,x_2,x_3)}-\\min{(x_1,x_2,x_3)}\\big)\\big(\\max{(y_1,y_2,y_3)}-\\min{(y_1,y_2,y_3)}\\big)]\\) where \\(x_i ~ U(0,1)\\), \\(y_i ~ U(0,1)\\) and \\(x_i, y_i\\) are independent. Thus, expected area can be simplified to \\(E[\\big(\\max{(x_1,x_2,x_3)}-\\min{(x_1,x_2,x_3)}\\big)]^2\\) Let \\(\\max{(x_1,x_2,x_3)\\) be represents as \\(x_{(1)}\\) and \\(\\min{(x_1,x_2,x_3)\\) as \\(x_{(3)}\\) Then \\(P_{X_{(1)}}(X_{(1)} &lt; x) = P(X_{1}&lt;x)P(X_{2}&lt;x)P(X_{3}&lt;x) = x^3 \\implies E[X_{(1)}]=\\frac{3}{4}\\) Similarly, \\(P_{X_{(3)}}(X_{(3)} &lt; x) = 1-(1-x)^3 \\implies P_{X_{(3)}}(X_{(3)} = x) = 3(1-x)^2 \\implies E[X_{(3)}]=\\frac{1}{4}\\) Thus, expected area = \\((3/4-1/4)^2= 1/16\\) 4.3 Problem 2 \\(f(x,y) g(\\sqrt{x^2+y^2})\\) Consider the transformation: \\[ \\begin{align*} x &amp;= r \\cos(\\theta)\\\\ y &amp;= r \\sin(\\theta) \\end{align*} \\] Then \\(f(r \\cos(\\theta), r \\sin(\\theta)) = g(r)\\) Reverse tranformation gives: \\[ \\begin{align*} Z &amp;= X/Y = \\tan(\\theta)\\\\ r^2 &amp;= y^2\\sec^2(\\theta) \\end{align*} \\] Since \\(f(r, \\theta) = g(r)\\) is independnet of \\(\\theta\\), \\(\\theta ~ U\\). Let \\(\\theta ~ U(0,2\\pi)\\) Now, \\(\\theta = \\tan^{-1}((Z)\\) \\(f_Z(z) = f_\\theta(\\tan^{-1}(z)) |\\frac{\\partial \\theta}{\\partial z}|+ f_\\theta(\\pi+\\tan^{-1}(z)) |\\frac{\\partial \\theta}{\\partial z}| = \\frac{1}{2\\pi}\\) \\(\\frac{\\partial \\theta}{\\partial z} = 1/sec^2(\\theta) = \\frac{1}{z^2+1}\\) Thus, $f_Z(z) = = 4.4 Problem 3 4.5 Part a \\(E[X_{n+1}^r|X_n] = \\int_0^{cx_n} x^r \\frac{1}{cx_n} dx = \\frac{(cx_n)^r}{r+1}\\) 4.6 Part b For \\(r=1\\): \\(E[X_{n+1}|X_n] = \\frac{cx_n}{2}\\) $EX_{n+1}= E[E[X_{n+1}|X_n] ]= $ Thus $ = ^n 0 $ as \\(n \\rightarrow \\infty\\) as &lt; 1$ For \\(r=2\\): $EX_{n+1}^2 = $ $ = ^n $ as \\(n \\rightarrow \\infty\\) 4.7 Part c 4.7.1 ToDO 4.8 Problem 4 4.8.1 Part a All \\(n\\) boys in single block \\(\\implies\\) rreat them as one unit. Now for \\(m\\) girls there are \\(m+1\\) spots where we can put this one ‘unit’ in \\(m+1\\) ways while the girls can be arranged in \\(m!\\) ways and among the unit the boys can be arranged in \\(n!\\) ways. Hence, the required probability: \\(\\frac{(m+1)n!m!}{(n+m)!}\\) 4.8.2 Part b For \\(n&gt;m\\) the probability is zero, If \\(n\\leq m\\) we arrange the girls first leaving \\(m+1\\) spaces for \\(n\\) boys which can be filled in \\({m+1 \\choose n} \\times n!\\) and the girls can be arranged in \\(m!\\) ways Required probability: \\(\\frac{{m+1 \\choose n}m!n!}{(n+m)!}\\) 4.8.3 Part c Define: \\(I_i=1\\) if \\((i-1,i,i+1)=(g,b,g)\\) and \\(0\\) otherwise. Then \\(EW = E\\sum_{i=1}^{n+m-2} I_i = (n+m-2) EI_1 = (n+m-2)P(I_1=1) = (n+m-2) \\frac{n(m)(m-1)}{(n+m)!}\\) "]
]
