[
["index.html", "MATH-505A Screening Solutions", " MATH-505A Screening Solutions Solutions to MATH-505A screening exams are provided. They are indexed by (year, semester). "],
["spring.html", "1 2006-Spring 1.1 Paper 1.2 Problem 1 1.3 Problem 2 1.4 Problem 3", " 1 2006-Spring 1.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20061/505aspring06.pdf 1.2 Problem 1 Define an indicator variable \\(I_i\\) as: \\[ I_i = \\begin{cases} 1 &amp; \\text{if $i^{th}$ and $(i+1)^{th}$ cards are different (H,T) or (T,H)},\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Now the number of runs in a sequence of \\(n\\) coin tosses is given by: \\[ R_n = 1+ \\sum_{i=2}^{n-1} I_i \\forall n\\geq 3 \\] Thus, \\[ \\begin{align} ER_n &amp;= 1 + \\sum_{i=2}^{n-1} EI_i \\\\ &amp;= 1 + \\sum_{i=2}^{n-1} P(I_i=1) \\end{align} \\] \\(P(I_i=1)\\) is given by : \\(P(I_i=1)=p\\times q + q \\times p\\) (heads followed by tails or tails followed by heads) And hence, \\[ ER_n = 1+(n-2) \\times (2pq) \\] Check: - \\(ER_1 = 1\\) and that is \\(ER_1=1\\) - \\(P(R_2=1)=p^2 + q^2\\) and \\(P(R_2=2)=pq+qp=2pq\\) thus \\(E(R_2)=(p^2+q^2)+4pq = (p+q)^2+2pq = 1+2pq\\) which is what \\(ER_n\\) formula gives us for n=2 1.2.1 Variance Calculation: To calculate: \\(\\sigma^2=Var(R_n)\\) \\(Var(R_n) = E(R_n^2)-(ER_n)^2\\) So we focus on calculating \\(ER_n^2\\): \\[\\begin{align} ER_n^2 &amp;= E((1+\\sum_{i=2}^{n-1}I_i)^2)\\\\ &amp;= E(1+(\\sum_{i=1}^{n-1}I_i)^2 + 2\\sum_{i=2}^{n-1}I_i))\\\\ &amp;= E(1+(\\sum{i=2}^{n-1}I_i^2 + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j) + 2\\sum_{i=2}^{n-1}I_i)\\\\ &amp;= 1 + (n-2)\\times(2pq) + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j + 2(n-2)(2pq) \\\\ &amp;= 1+3(n-2)\\times(2pq) + 2\\sum_{2\\leq i &lt; j}^{n-1} I_iI_j\\\\ \\end{align}\\] In order to calculate \\(EI_iI_j\\), we consider following 3 cases: Case 1: \\(j-i=1\\), then \\(P(I_i=1, I_j=1)\\) = Probaility that \\(i^{th}, (i+1)^{th} \\mathrm{and} (i+2)^{th}\\) cards are different. For \\(i=2\\) to \\(i={n-1}\\) there are \\((n-2-1)=n-3\\) such terms and \\(P(I_i,I_j=1)= pqp+qpq=pq\\) Case 2: \\(j-i&gt;=2\\), then \\(P(I_i=1,I_j=1) = P(I_i=1)P(I_j=1)\\) , that is these events are independent unless they occur next to each other as in Case 1. and hence \\(P(I_i=1,I_j=1)=P(I_i=1)P(I_j=1) = (pq)^2\\) and there are \\((n-4)\\) such ways to choose Thus, \\(ER_n^2 = 1+6pq(n-2)+2\\times((n-3)\\times (pq)+(n-4)\\times(pq)^2)\\) Substitute in (skipping/TODO) \\(Var(R_n) = ER_n^2-(ER_n)^2\\) 1.3 Problem 2 \\(X = \\{X_1,X_2 \\dots, X_n \\}\\) and \\(Y=\\sum_{i=1}^{N}c_iX_i\\) To determine : - Distribution of Y We use characteristic functions Using the characteristic function of a normal RV: \\(\\phi_X(t) = E[e^{itX}] = e^{-it\\mu- \\frac{1}{2}\\sigma^2t^2}\\) For multivariate case: \\(\\phi_X(\\bf{t}) = E[e^{i\\bf{t^T}X}] = e^{-i\\bf{t^T}\\mu- \\frac{1}{2}\\bf{t^T}\\sum^2 \\bf{t}}\\) Now, \\(Y=c^TX\\) where \\(c=[c_1,c_2 \\dots c_n]\\) (\\(Y=\\sum_{i=1}^{N}c_iX_i\\)) Thus, \\[ \\begin{align} \\phi_Y(\\bf{t}) &amp;= E[e^{i\\bf{t^T}Y}]\\\\ &amp;= E[e^{i\\bf{t}c^TX}]\\\\ &amp;= \\phi_X(tc^T)\\\\ &amp;= e^{-ic^T\\bf{t}\\mu - \\frac{1}{2}\\bf{t}^Tc\\sum^2 \\bf{t}c^T} \\end{align} \\] and thus comparing with the characteristic function we started with: \\(Y \\sim N(c^T\\mu, a\\sum a^T)\\) TODO: Check again the transposes 1.4 Problem 3 TODO "],
["spring-1.html", "2 2013-Spring 2.1 Paper 2.2 Problem 1", " 2 2013-Spring 2.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20061/505aspring06.pdf 2.2 Problem 1 Given: \\(E[X|Y]=X\\ and \\ E[Y|X]=X\\) To Show: 2.2.1 Part (a): \\(P(X=Y)=1\\) \\[ E[Y]=E[E[Y|X]]=E[X]=\\mu_x \\] Thus, \\(\\mu_y=\\mu_x\\) Also, \\[ \\begin{align} Cov(X,Y) &amp;=E[XY]-E[X]E[Y]\\\\ &amp;= E[E[XY|X]]-\\mu_x^2\\\\ &amp;= E[XE[Y|X]]-\\mu_x^2\\\\ &amp;= E[X^2]-\\mu_x^2\\\\ &amp;= \\sigma_x^2 \\end{align} \\] Repeating the above with \\(E[XY]= E[E[XY|Y]]\\) would give \\(Cov(X,Y)=\\sigma_y^2\\) and hence \\(Cov(X,Y)=\\sigma_x^2=\\sigma_y^2=Var(X)\\) which implies \\(X=Y\\) or \\(P(X=Y)=1\\) Note, we implicitly used the requirement of the variance being finite[This is what is implied by the function being squared integrable: \\(\\int |f(X)|^2dx &lt; \\infty\\) "],
["spring-2.html", "3 2014-Spring 3.1 Paper 3.2 Problem 1 3.3 Problem 2", " 3 2014-Spring 3.1 Paper http://www-bcf.usc.edu/~mathgp/quals/20141/20141_505a.pdf 3.2 Problem 1 Define \\[I_i = \\begin{cases} 1 &amp; \\text{Game $i$ and Game $i+1$ are won},\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Thus, \\[ \\begin{align*} R &amp;= \\sum_{i=1}^{m-1}I_i\\\\ ER &amp;= \\sum_{i=1}^{m-1}EI_i\\\\ Var(R) &amp;= \\sum_{i=1}^{m-1}Var(I_i) + 2\\sum_{i &lt; j}Cov(I_i,U) \\end{align*} \\] 3.2.1 Part 1.a \\[ \\begin{align*} EI_i &amp;= P(I_i=1) \\\\ &amp;= a_ia_{i+1}\\\\ ER &amp;= \\sum_i EI_i\\\\ &amp;=\\sum_{i=1}^{m-1}a_ia_{i+1} \\end{align*} \\] \\[ \\begin{align*} Var(I_i) &amp;= E[I_i^2]-E[I_i]E[I_i]\\\\ &amp;= E[I_i](1-E[I_i])\\\\ &amp;= a_ia_{i+1}(1-a_ia_{i+1}) \\end{align*} \\] \\[ \\begin{align*} Cov(I_i,I_j) &amp;= \\begin{cases} 0 &amp; j-i\\geq 2(i&lt;j)\\\\ E[I_{i}I_{i+1}]-E[I_i]E[I_{i+1}] &amp; \\text{otherwise}\\\\ \\end{cases}\\\\ &amp;= \\begin{cases} 0 &amp; j-i\\geq 2(i&lt;j)\\\\ a_ia_{i+1}a_{i+2}(1-a_{i+1}) &amp; \\text{otherwise}\\\\ \\end{cases}\\\\ \\end{align*} \\] Thus, \\[ Var(R) = \\sum_{i=1}^{m-1}a_ia_{i+1} + 2\\sum_{i=1}^{m-2} a_ia_{i+1}a_{i+2}(1-a_{i+1}) \\] 3.2.2 Part 1.b When \\(a_n=p=0.1\\) \\[ \\begin{align*} ER &amp;= \\sum_{i=1}^{m-1}p^2\\\\ &amp;= (m-1)p^2\\\\ Var(R) &amp;= \\sum_{i=1}^{m-1}a_ia_{i+1} + 2\\sum_{i=1}^{m-2} a_ia_{i+1}a_{i+2}(1-a_{i+1})\\\\ &amp;= (m-1)p^2+2(m-2)p^3(1-p) \\end{align*} \\] 3.3 Problem 2 \\[f(x) = x/2 \\text{ for } 0 &lt; x &lt;2\\] 3.3.1 Part 2.a \\(S=X_1+X_2\\) Using convolution theorem: \\[ \\begin{align*} f_S(s) &amp;= \\int \\frac{x}{2} \\frac{s-x}{2} dx\\\\ &amp;0 &lt; s-x &lt; 2\\\\ &amp; 0 &lt;x&lt;2\\\\ &amp;\\implies s-2 &lt; x &lt; s \\text{ and } 0 &lt; x &lt; 2\\\\ &amp;\\text{Case 1: } 0 &lt; s &lt; 2\\ f_S(s) = \\int_0^s \\frac{x}{2} \\frac{s-x}{2} dx = \\frac{s^3}{24}\\\\ &amp;\\text{ Case 2: } 2 &lt; s &lt; 4\\ f_S(s) = \\int_{s-2}^4 \\frac{x}{2} \\frac{s-x}{2} dx = \\frac{1}{4}(\\frac{s(4s-s^2)}{2}-\\frac{(4^3-(s-2)^3)}{3})\\\\ \\end{align*} \\] 3.3.2 Part 2.b \\[L = min(X_1, X_2, X_3, \\dots X_{100})\\] $F_L = 1 - P(min(X_i) y) = 1 - _i P(X_i y) = 1-(1-P(X_iy)) = 1- (1-)^n $ 3.3.3 Part 2.c \\[R=X_1/X_2\\] \\[ \\begin{align*} P(X_1/X_2 \\leq z) &amp;= P(X_1 \\leq zX_2)\\\\ &amp;= \\int_0^{min(zx_2,2)} \\frac{z^2x_2^2}{4}x_2 dx_2 \\end{align*} \\] "]
]
